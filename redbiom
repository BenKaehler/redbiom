#!/usr/bin/env python

import click


def _get_config():
    """Deal with all the configy bits"""
    import os
    import requests.auth
    user = os.environ.get('SEQUENCE_SEARCH_USER')
    password = os.environ.get('SEQUENCE_SEARCH_PASSWORD')
    hostname = os.environ.get('SEQUENCE_SEARCH_HOST', 'http://127.0.0.1:7379')

    if user is None:
        auth = None
    else:
        auth = requests.auth(config['user'], config['password'])
    return {'auth': auth, 'hostname': hostname}


def _float_or_nan(t):
    import math
    try:
        return float(t)
    except:
        return math.nan


def _parse_validate_request(req, command):
    import requests
    if req.status_code != 200:
        raise requests.HTTPError("%s : %s" % (command, req.content))
    return req.json()[command]


def _format_request(command, other):
    return "%s/%s" % (command, other)


def _make_post(config):
    import requests
    s = requests.Session()
    s.auth = config['auth']
    def f(cmd, payload):
        req = s.post(config['hostname'], data=_format_request(cmd, payload))
        return _parse_validate_request(req, cmd)
    return f


def _make_put(config):
    import requests
    s = requests.Session()
    s.auth = config['auth']
    def f(cmd, key, data):
        url = '/'.join([config['hostname'], _format_request(cmd, key)])
        req = s.put(url, data=data)
        return _parse_validate_request(req, cmd)
    return f


def _make_get(config):
    import requests
    s = requests.Session()
    s.auth = config['auth']
    def f(cmd, data):
        payload = _format_request(cmd, data)
        url = '/'.join([config['hostname'], payload])
        return _parse_validate_request(s.get(url), cmd)
    return f


def _indexable(value, nullables):
    """Returns true if the value appears to be something that storable"""
    if value in nullables:
        return False

    if isinstance(value, (float, int, bool)):
        return True
    else:
        return '/' not in value


def _from_or_nargs(from_, nargs_variable):
    """In support of _buffered: determine whether to use from_ or nargs"""
    import sys
    if from_ is None and not nargs_variable:
        click.echo('Need at least 1 item', err=True)
        sys.exit(1)  # should be doable from click but need ctx i think...?

    if from_ is not None and nargs_variable:
        click.echo("Unable to handle --from as well as cmdline itemss",
                   err=True)
        sys.exit(1)
    
    if from_ is not None:
        nargs_variable = from_

    return iter(nargs_variable)


def _buffered(it, prefix, cmd, get=None, buffer_size=10, multikey=None):
    """Bulk fetch data"""
    if get is None:
        config = _get_config()
        get = _make_get(config)
    
    if multikey is None:
        prefixer = lambda a, b: '%s:%s' % (a, b)
    else:
        prefixer = lambda a, b: b

    exhausted = False
    while not exhausted:
        items = []
        for i in range(buffer_size):
            try:
                items.append(next(it).strip())
            except StopIteration:
                exhausted = True
                break
        
        bulk = '/'.join([prefixer(prefix, i) for i in items])
        if multikey:
            bulk = "%s/%s" % (multikey, bulk)
        yield items, get(cmd, bulk)


def _biom_from_samples(samples, output, get=None):
    """Create a BIOM table from an iterable of samples"""
    import json
    from operator import itemgetter
    import scipy.sparse as ss
    import biom
    import h5py

    if get is None:
        config = _get_config()
        get = _make_get(config)

    # pull out the observation index so the IDs can be remapped
    obs_index = json.loads(get('GET', '__observation_index'))

    # redis contains {observation ID -> internal ID}, and we need
    # {internal ID -> observation ID}
    inverted_obs_index = {v: k for k, v in obs_index.items()}

    # pull out the per-sample data
    table_data = []
    unique_indices = set()
    
    getter = _buffered(samples, 'data', 'MGET', get=get, buffer_size=100)
    for (sample_set, sample_set_data) in getter:
        for sample, data in zip(sample_set, sample_set_data):
            data = data.split('\t')
            table_data.append((sample, data))

            # update our perspective of total unique observations
            unique_indices.update({int(i) for i in data[::2]})

    # construct a mapping of {observation ID : index position in the BIOM table}
    unique_indices_map = {observed: index for index, observed in enumerate(unique_indices)}

    # pull out the observation and sample IDs in the desired ordering
    obs_ids = [inverted_obs_index[k] for k, _ in sorted(unique_indices_map.items(), key=itemgetter(1))]
    sample_ids = [d[0] for d in table_data]

    # fill in the matrix
    mat = ss.lil_matrix((len(unique_indices), len(table_data)))
    for col, (sample, col_data) in enumerate(table_data):
        # since this isn't dense, hopefully roworder doesn't hose us
        for index, value in zip(col_data[::2], col_data[1::2]):
            mat[unique_indices_map[int(index)], col] = value

    # write it out
    table = biom.Table(mat, obs_ids, sample_ids)
    with h5py.File(output, 'w') as fp:
        table.to_hdf5(fp, 'redbiom')


def _samples_from_observations(it, exact, get=None):
    """Grab samples from an iterable of observations"""
    cmd = 'SINTER' if exact else 'SUNION'
    samples = None
    for _, block in _buffered(it, 'samples', cmd, get=get):
        block = set(block)
        
        if not exact:
            if samples is None:
                samples = set()
            samples.update(block)
        else:
            if samples is None:
                samples = block
            else:
                samples = samples.intersection(block)
    return samples


def _summarize_samples(samples, category, value, get):
    """Summaryze an iterable of samples based on criteria"""
    key = 'category:%s' % category
    getter = _buffered(iter(samples), None, 'HMGET', get=get, buffer_size=100, 
                       multikey=key)
    
    results = []
    for samples, category_values in getter:
        for sample, observed_value in zip(samples, category_values):
            results.append((sample, observed_value))
   
    if value is None:
        from collections import Counter
        from operator import itemgetter
        
        cat_stats = Counter([v for s, v in results])
        for val, count in sorted(cat_stats.items(), key=itemgetter(1), 
                                 reverse=True):
            click.echo("%s\t%s" % (val, count))
        click.echo("\n%s\t%s" % ("Total samples", len(samples)))
    else:
        import shlex
        tokens = list(shlex.shlex(value))
        if len(tokens) > 1:
            # < 5
            # in "foo bar",blah
            # notin thing,"other thing"

            op = {'in': lambda a, b: a in b,
                  'notin': lambda a, b: a not in b,
                  '<': lambda a, b: a <= b,
                  '>': lambda a, b: a >= b}
            operator = op.get(tokens[0])
            if operator is None:
                raise ValueError("Cannot parse: %s" % value)
            if tokens[0] in ('in', 'notin'):
                rh = [t.strip("'").strip('"') for t in tokens[1:] if t != ',']
                tokens = set(rh)
                func = lambda to_test: operator(to_test, rh)
            else:
                rh = tokens[1]
                if len(tokens) > 2:
                    raise ValueError("Unexpected additional criteria: %s" % value)
                try:
                    rh = float(rh)
                except TypeError:
                    if operator in {'<=', '>='}:
                        raise ValueError("Right hand does not look numeric")

                func = lambda to_test: operator(_float_or_nan(to_test), rh)
        else:
            func = lambda to_test: to_test == value
        click.echo("\n".join([s for s, v in results if func(v)]))


@click.group()
def cli():
    pass


@cli.group()
def admin():
    """Update database, etc."""
    pass


@cli.group()
def search():
    """Observation and sample search support."""
    pass


@cli.group()
def fetch():
    """Sample data and metadata retrieval."""
    pass


@cli.group()
def summarize():
    """Summarize things."""
    pass


@admin.command(name='load-observations')
@click.option('--table', required=True, type=click.Path(exists=True))
def load_observations(table):
    """Load observation to sample mappings.

    For each observation, all samples in the table associated with the
    observation are added to a Redis set keyed by "samples:<observation_id>".
    """
    import biom

    config = _get_config()
    post = _make_post(config)

    tab = biom.load_table(table)
    samples = tab.ids()[:]

    for values, id_, _ in tab.iter(axis='observation', dense=False):
        observed = samples[values.indices]
        payload = "samples:%s/%s" % (id_, "/".join(observed))
        post('SADD', payload)


@admin.command(name='load-sample-data')
@click.option('--table', required=True, type=click.Path(exists=True))
def load_sample_data(table):
    """Load nonzero entries per sample.

    WARNING: this method does not support non count data.

    The observation IDs are remapped into an integer space to reduce memory
    consumption as sOTUs are large. The index is maintained in Redis.

    The indexing scheme requires a central authority for obtaining a unique
    and stable index value. As such, this method obtains a lock for performing
    an update. The index is updated with any new observations seen on each
    load and is stored under the "__observation_index" key. It is stored as
    a JSON object mapping the original observation ID to a stable and unique
    integer; stability and uniqueness is not assured across distinct redis
    databases.

    The data are stored per sample with keys of the form "data:<sample_id>".
    The string stored is tab delimited, where the even indices (i.e .0, 2, 4,
    etc) correspond to the unique index value for an observation ID, and the
    odd indices correspond to the counts associated with the sample/observation
    combination.
    """
    import biom
    import time
    import json

    config = _get_config()
    post = _make_post(config)
    get = _make_get(config)

    tab = biom.load_table(table)
    samples = tab.ids()[:]
    obs = tab.ids(axis='observation')

    acquired = False
    while not acquired:
        # not using redlock as time interval isn't that critical
        acquired = get('SETNX', '__load_table_lock/1') == 1
        if not acquired:
            click.echo("%s is blocked" % table)
            time.sleep(1)

    try:
        # load the observation index
        obs_index = get('GET', '__observation_index')
        if obs_index is None:
            obs_index = {}
        else:
            obs_index = json.loads(obs_index)

        # update the observation index if and where necessary
        new_ids = {i for i in obs if i not in obs_index}
        id_start = max(obs_index.values() or [-1]) + 1
        for idx, id_ in zip(range(id_start, id_start + len(new_ids)), new_ids):
            obs_index[id_] = idx

        # load up the table per-sample
        for values, id_, _ in tab.iter(dense=False):
            int_values = values.astype(int)
            remapped = [obs_index[i] for i in obs[values.indices]]

            packed = '\t'.join(["%s\t%d" % (i, v)
                                for i, v in zip(remapped,
                                                int_values.data)])
            post('SET', 'data:%s/%s' % (id_, packed))

        # store the index following the load of the table
        post('SET', '__observation_index/%s' % json.dumps(obs_index))

    finally:
        # release the lock no matter what
        get('DEL', '__load_table_lock')


@admin.command(name='load-sample-metadata')
@click.option('--metadata', required=True, type=click.Path(exists=True))
def load_sample_metadata(metadata):
    """Load sample metadata."""
    import pandas as pd
    import json

    config = _get_config()
    post = _make_post(config)
    put = _make_put(config)

    md = pd.read_csv(metadata, sep='\t', dtype=str).set_index('#SampleID')

    null_values = {'Not applicable', 'Unknown', 'Unspecified',
                   'Missing: Not collected',
                   'Missing: Not provided',
                   'Missing: Restricted access',
                   'null', 'NULL', 'no_data', 'None', 'nan'}

    indexed_columns = md.columns
    for idx, row in md.iterrows():
        # denote what columns contain information
        columns = [c for c, i in zip(md.columns, row.values) 
                   if _indexable(i, null_values)]
        key = "metadata-categories:%s" % idx

        # TODO: express metadata-categories using redis sets
        # TODO: dumps is expensive relative to just, say, '\t'.join
        put('SET', key, json.dumps(columns))

    for col in indexed_columns:
        bulk_set = ["%s/%s" % (idx, v) for idx, v in zip(md.index, md[col])
                    if _indexable(v, null_values)]

        payload = "category:%s/%s" % (col, '/'.join(bulk_set))
        post('HMSET', payload)


@fetch.command(name='sample-metadata')
@click.option('--table', required=False, type=click.Path(exists=True))
@click.option('--from', 'from_', type=click.File('r'), required=False, 
              default=None)
@click.option('--output', required=True, type=click.Path(exists=False))
@click.argument('samples', nargs=-1)
def fetch_sample_metadata(table, from_, samples, output):
    """Retreive sample metadata."""
    if table is not None:
        if from_ is not None or samples:
            click.echo("Cannot specify --table with --from or cmdline samples",
                       err=True)
            import sys
            sys.exit(1)
        
        import h5py
        it = iter(h5py.File(table)['sample/ids'][:])
    else:
        it = _from_or_nargs(from_, samples)

    import json
    from collections import defaultdict
    import pandas as pd

    config = _get_config()
    get = _make_get(config)

    # TODO: express metadata-categories using redis sets
    # and then this can be done with SINTER
    all_columns = []
    all_samples = []
    getter = _buffered(it, 'metadata-categories', 'MGET', get=get, 
                       buffer_size=100)
    for samples, columns_by_sample in getter:
        all_samples.extend(samples)
        for column_set in columns_by_sample:
            if column_set is not None:
                column_set = json.loads(column_set)
                all_columns.append(set(column_set))
    
    common_columns = set(all_columns[0])
    for columns in all_columns[1:]:
        common_columns = common_columns.intersection(columns)
    
    metadata = defaultdict(dict)
    for sample in all_samples:
        metadata[sample]['#SampleID'] = sample

    for category in common_columns:
        key = 'category:%s' % category
        getter = _buffered(iter(all_samples), None, 'HMGET', get=get, 
                           buffer_size=100, multikey=key)

        for samples, category_values in getter:
            for sample, value in zip(samples, category_values):
                metadata[sample][category] = value
    
    md = pd.DataFrame(metadata).T
    md.to_csv(output, sep='\t', header=True, index=False)


@fetch.command(name='observations')
@click.option('--from', 'from_', type=click.File('r'), required=False, 
              default=None)
@click.option('--output', required=True, type=click.Path(exists=False))
@click.option('--exact', is_flag=True, default=False,
              help="All found samples must contain all specified observations")
@click.argument('observations', nargs=-1)
def fetch_samples_from_obserations(observations, exact, from_, output):
    """Fetch sample data containing observations."""
    it = _from_or_nargs(from_, observations)

    config = _get_config()
    get = _make_get(config)

    # determine the samples which contain the observations of interest
    samples = _samples_from_observations(it, exact, get=get)
    
    _biom_from_samples(iter(samples), output, get=get)


@fetch.command(name='samples')
@click.option('--from', 'from_', type=click.File('r'), required=False, 
              default=None)
@click.option('--output', required=True, type=click.Path(exists=False))
@click.argument('samples', nargs=-1)
def fetch_samples_from_samples(samples, from_, output):
    """Fetch sample data."""
    it = _from_or_nargs(from_, samples)
    _biom_from_samples(it, output)


@search.command(name="observations")
@click.option('--from', 'from_', type=click.File('r'), required=False, 
              default=None)
@click.option('--exact', is_flag=True, default=False,
              help="All found samples must contain all specified observations")
@click.argument('observations', nargs=-1)
def search_observations(from_, exact, observations):
    """Find samples containing observations."""
    it = _from_or_nargs(from_, observations)

    # determine the samples which contain the observations of interest
    samples = _samples_from_observations(it, exact)

    for sample in samples:
        click.echo(sample)


@search.command(name="samples")
@click.option('--category', type=str, required=True)
@click.option('--operator', required=True,
              type=click.Choice(['eq', 'ne', 'in', 'lt', 'gt']))
@click.option('--value', type=str, required=True)
def samples(category, operator, value):
    """Find samples given metadata criteria."""
    config = _get_config()
    get = _make_get(config)

    op = {'eq': lambda a, b: a == b,
          'ne': lambda a, b: a != b,
          'in': lambda a, b: a in b,
          'lt': lambda a, b: a < b,
          'gt': lambda a, b: a > b}[operator]

    try:
        value = float(value)
    except:
        pass

    if operator == 'in':
        value = value.split(',')
        try:
            value = [float(v) for v in value]
        except:
            pass

    import time
    start = time.time()
    sample_values = get('HGETALL', 'category:%s' % category)
    print(time.time() - start)

    import pandas as pd
    for sample, obs_value in sample_values.items():
        try:
            obs_value = float(obs_value)
        except:
            pass
        try:

            if op(obs_value, value):
                click.echo("%s\t%s" % (sample, str(obs_value)))
                pass
        except:
            pass


@summarize.command(name='observations')
@click.option('--from', 'from_', type=click.File('r'), required=False, 
              default=None)
@click.option('--category', type=str, required=True)
@click.option('--value', type=str, required=False, default=None, 
              help="Restrict to a specific value; prints the sample IDs")
@click.option('--exact', is_flag=True, default=False,
              help="All found samples must contain all specified observations")
@click.argument('observations', nargs=-1)
def summarize_observations(from_, category, exact, value, observations):
    """Summarize observations over a metadata category."""
    it = _from_or_nargs(from_, observations)

    config = _get_config()
    get = _make_get(config)

    samples = _samples_from_observations(it, exact, get=get)
    _summarize_samples(samples, category, value, get)


@summarize.command(name='samples')
@click.option('--from', 'from_', type=click.File('r'), required=False, 
              default=None)
@click.option('--category', type=str, required=True)
@click.option('--value', type=str, required=False, default=None, 
              help="Restrict to a specific value; prints the sample IDs")
@click.argument('samples', nargs=-1)
def summarize_samples(from_, category, value, samples):
    """Summarize samples over a metadata category."""
    it = _from_or_nargs(from_, samples)
    _summarize_samples(it, category, value, get)


if __name__ == '__main__':
    cli()
